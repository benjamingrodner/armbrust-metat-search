{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get estcounts for each taxon at each sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gc\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "import math\n",
    "import umap\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from collections import * \n",
    "from sklearn import cluster\n",
    "from sklearn import decomposition\n",
    "from ete4 import NCBITaxa, Tree\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as pltc\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../repo-armbrust-metat-search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions.fn_metat_files as fnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi = NCBITaxa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '/scratch/bgrodner/p_calceolata_enterobactin'\n",
    "os.chdir(workdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_plot(\n",
    "    xlabel=\"\", ylabel=\"\", ft=12, dims=(5, 3), col=\"k\", lw=1, pad=0, tr_spines=True\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(dims[0], dims[1]), tight_layout={\"pad\": pad})\n",
    "    for i in ax.spines:\n",
    "        ax.spines[i].set_linewidth(lw)\n",
    "    if not tr_spines:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "    else:\n",
    "        ax.spines[\"top\"].set_color(col)\n",
    "        ax.spines[\"right\"].set_color(col)\n",
    "    ax.spines[\"bottom\"].set_color(col)\n",
    "    ax.spines[\"left\"].set_color(col)\n",
    "    ax.tick_params(direction=\"in\", labelsize=ft, color=col, labelcolor=col)\n",
    "    ax.set_xlabel(xlabel, fontsize=ft, color=col)\n",
    "    ax.set_ylabel(ylabel, fontsize=ft, color=col)\n",
    "    ax.patch.set_alpha(0)\n",
    "    return (fig, ax)\n",
    "\n",
    "def plot_umap(\n",
    "    embedding,\n",
    "    figsize=(10, 10),\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    colors=\"k\",\n",
    "    xticks=[],\n",
    "    yticks=[],\n",
    "    markerstyle='o',\n",
    "    cmap_name='tab20',\n",
    "    cl_lab=False\n",
    "):\n",
    "    fig, ax = general_plot(dims=figsize)\n",
    "    if isinstance(markerstyle, str):\n",
    "        ax.scatter(\n",
    "            embedding[:, 0],\n",
    "            embedding[:, 1],\n",
    "            s=markersize,\n",
    "            alpha=alpha,\n",
    "            c=colors,\n",
    "            edgecolors=\"none\",\n",
    "            marker=markerstyle,\n",
    "            cmap=cmap_name\n",
    "        )\n",
    "    else:\n",
    "        for e0, e1, c, m in zip(\n",
    "            embedding[:, 0], \n",
    "            embedding[:, 1],\n",
    "            colors,\n",
    "            markerstyle \n",
    "        ):\n",
    "            ax.scatter(\n",
    "                e0,\n",
    "                e1,\n",
    "                s=markersize,\n",
    "                alpha=alpha,\n",
    "                c=c,\n",
    "                edgecolors=\"none\",\n",
    "                marker=m\n",
    "            )\n",
    "    ax.set_aspect(\"equal\")\n",
    "    if len(xticks) > 0:\n",
    "        ax.set_xticks(xticks)\n",
    "    if len(yticks) > 0:\n",
    "        ax.set_yticks(yticks)\n",
    "    ax.set_xlabel(\"UMAP 1\")\n",
    "    ax.set_ylabel(\"UMAP 2\")\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get KO dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_fn = \"../iron_ko_contigs/ko00001.json\"\n",
    "database = list()\n",
    "for _, v in pd.read_json(ko_fn).iterrows():\n",
    "    d = v[\"children\"]\n",
    "    cat_1 = d[\"name\"]\n",
    "    for child_1 in d[\"children\"]:\n",
    "        cat_2 = child_1[\"name\"] # Module?\n",
    "        for child_2 in child_1[\"children\"]:\n",
    "            cat_3 = child_2[\"name\"]\n",
    "            if \"children\" in child_2:\n",
    "                for child_3 in child_2[\"children\"]:\n",
    "                    cat_4 = child_3[\"name\"]\n",
    "                    fields = [cat_1, cat_2, cat_3, cat_4]\n",
    "                    database.append(fields)\n",
    "df_kegg = pd.DataFrame(database, columns=[\"Level_A\", \"Level_B\", \"Level_C\", \"Level_D\"])\n",
    "df_kegg.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = df_kegg['Level_D'].values\n",
    "ld[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ko_name = {}\n",
    "for name in ld:\n",
    "    ko = re.search(r\"^\\w+\",name)[0]\n",
    "    dict_ko_name[ko] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/scratch/bgrodner/iron_ko_contigs/metat_search_results/dicts_iron_KO_contig/tidy_tables/merge_all/iron_KOs.txt-metadata.csv\"  # input('Enter the filepath of your batch metadata file:')\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.iloc[[1,100,200],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct G1 S11C1 latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats_new = []\n",
    "for i, row in metadata.iterrows():\n",
    "    if (row['sample'] == 'S11C1') & ('G1' in row['assembly']):\n",
    "        lats_new.append('36.569deg')\n",
    "    else:\n",
    "        lats_new.append(row.latitude)\n",
    "metadata['latitude'] = lats_new\n",
    "metadata.iloc[[1,100,200],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load contig taxon map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_contig_tax = '/mnt/nfs/projects/armbrust-metat/gradients2/g2_station_ns_metat/assemblies/MarMicro_MarFerr_Diamond_2024_04_14/G2NS_ALL_MarFer_MMDB.tab'\n",
    "with open(fn_contig_tax, 'r') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_contig_tax = {}\n",
    "with open(fn_contig_tax, 'r') as f:\n",
    "    for row in f:\n",
    "        contig, ec, _ = row.split('\\t')\n",
    "        dict_contig_tax[contig] = ec\n",
    "\n",
    "print(f'{len(dict_contig_tax):,} lines read')\n",
    "fnf.getmem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add counts to taxon for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_kallisto = '/mnt/nfs/projects/armbrust-metat/gradients2/g2_station_ns_metat/assemblies/ReadCounts/'\n",
    "fns_kallisto = glob.glob(f'{dir_kallisto}/*/*.tsv')\n",
    "fns_kallisto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fns_kallisto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sam_tax_estcounts = defaultdict(lambda: defaultdict(float))\n",
    "for fn in fns_kallisto[:1]:\n",
    "    sam = os.path.split(fn)[1]\n",
    "    sam = os.path.splitext(sam)[0]\n",
    "    print(sam)\n",
    "    with open(fn, 'r') as f:\n",
    "        _ = next(f)  # skip header\n",
    "        for i, row in enumerate(f):\n",
    "            contig, _, _, ec, _ = row.split('\\t')\n",
    "            ec = float(ec)\n",
    "            if ec:\n",
    "                tax = dict_contig_tax[contig]\n",
    "                dict_sam_tax_estcounts[sam][tax] += ec\n",
    "            if i%1e6 == 0:\n",
    "                print(f'{i:,} lines read', end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster to load each contig tax dict multiple times in parallel or single time and then get each sample in series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = 24.8 + 60*26.4\n",
    "tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to parallel as cores increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.arange(1,100)\n",
    "tp = np.ceil(500/j) * (24.8 + 26.4)\n",
    "tp\n",
    "plt.scatter(j, tp)\n",
    "plt.plot([0,100],[tl]*2, 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapse counts to the trimmed tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tree trim file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_tree_trim = '/scratch/bgrodner/iron_ko_contigs/metat_search_results/dicts_iron_KO_contig/tree_trim/merge_all/iron_KOs.txt-barnacle_tensor_tidy-tree_trim_thresh_60_minsamples_20_minbatches_4.csv'\n",
    "with open(fn_tree_trim, 'r') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get set of taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_taxa = []\n",
    "with open(fn_tree_trim, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        list_taxa.append(row['taxon_trim'])\n",
    "set_taxa = set(list_taxa)\n",
    "set_taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ncbi.get_topology(set_taxa)\n",
    "\n",
    "print(tree.to_str(props=['sci_name', 'name'], compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All taxa counts filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_taxon_estcounts = '/scratch/bgrodner/relative_abundance/metat_search_results/sample_taxa_estcounts/G2NS/G2NS-G2NS.S18C1.15m.3um.C.tsv-count_sum.txt'\n",
    "with open(fn_taxon_estcounts, 'r') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tax_estcounts = {}\n",
    "with open(fn_taxon_estcounts, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for tax, ec in reader:\n",
    "        dict_tax_estcounts[tax] = float(ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_estcounts = 0\n",
    "for _, ec in dict_tax_estcounts.items():\n",
    "    total_estcounts += ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_untrim = list(dict_tax_estcounts.keys())\n",
    "tax_untrim.remove('0')\n",
    "tree_untrim = ncbi.get_topology(tax_untrim)\n",
    "print(tree_untrim.to_str(props=['sci_name','name'], compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get taxtrim dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_taxtrim_estcounts = defaultdict(float)\n",
    "set_tax_tree = [n.name for n in tree.traverse()]\n",
    "for tax, ec in dict_tax_estcounts.items():\n",
    "    if (tax in set_tax_tree) or (int(tax) == 0):\n",
    "        dict_taxtrim_estcounts[tax] += ec\n",
    "    else:\n",
    "        lin = ncbi.get_lineage(tax)\n",
    "        lin.reverse()\n",
    "        for t in lin:\n",
    "            t = str(t)\n",
    "            if t in set_tax_tree:\n",
    "                dict_taxtrim_estcounts[t] += ec\n",
    "                # ec_tot += ec\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_tot = 0\n",
    "for _, ec in dict_taxtrim_estcounts.items():\n",
    "    ec_tot += ec\n",
    "\n",
    "ec_tot / total_estcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add counts to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_tot = 0\n",
    "for n in tree.traverse():\n",
    "    ec = dict_taxtrim_estcounts.get(n.name)\n",
    "    if ec:\n",
    "        ec_tot += ec\n",
    "        n.add_props(estcounts=round(ec))\n",
    "\n",
    "print(ec_tot / total_estcounts)\n",
    "print(tree.to_str(props=['sci_name','estcounts'], compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add relative abundance to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_total = 0\n",
    "ec_total = 0\n",
    "for n in tree.traverse():\n",
    "    ec = dict_taxtrim_estcounts.get(n.name)\n",
    "    if ec:\n",
    "        ec_total += ec\n",
    "        pct = ec / total_estcounts * 100\n",
    "        pct_total += pct\n",
    "        n.add_props(pct_ec=f'{round(pct, 4)}%')\n",
    "print(pct_total, ec_total / total_estcounts)\n",
    "print(tree.to_str(props=['sci_name','pct_ec'], compact=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frac at root or unannotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = dict_tax_estcounts['0'] / total_estcounts * 100\n",
    "print(f'{round(pct , 5)}% unannotated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by which taxa have the most reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts = []\n",
    "taxs = []\n",
    "for tax, ec in dict_tax_estcounts.items():\n",
    "    pct = ec / total_estcounts * 100\n",
    "    pcts.append(pct)\n",
    "    taxs.append(tax)\n",
    "\n",
    "taxs = [x for _, x in sorted(zip(pcts,taxs), reverse=True)]\n",
    "pcts.sort(reverse=True)\n",
    "for i in range(10):\n",
    "    print(round(pcts[i],4), '%','\\t',taxs[i], ncbi.get_taxid_translator([taxs[i]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts = []\n",
    "taxs = []\n",
    "for tax, ec in dict_taxtrim_estcounts.items():\n",
    "    pct = ec / total_estcounts * 100\n",
    "    pcts.append(pct)\n",
    "    taxs.append(tax)\n",
    "\n",
    "taxs = [x for _, x in sorted(zip(pcts,taxs), reverse=True)]\n",
    "pcts.sort(reverse=True)\n",
    "print(sum(pcts))\n",
    "for i in range(len(taxs)):\n",
    "    print(round(pcts[i],4), '%','\\t',taxs[i], ncbi.get_taxid_translator([taxs[i]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cruise-sample profiles after running snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snakefile_all_taxa_estcounts -> a tidytable with all the relative abundance info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_relabund = '/scratch/bgrodner/relative_abundance/metat_search_results/sample_taxa_estcounts/merge-sample_taxid_estcounts-tidy.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map fnsamplecounts to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_meta = {}\n",
    "for i, row in metadata.iterrows():\n",
    "    dict_meta[row['fn_sample_counts']] = row.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add metadata to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_relabund = defaultdict(list)\n",
    "with open(fn_relabund, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Get metadata for sample\n",
    "        meta_row = dict_meta[row['fn_sample_counts']]\n",
    "        # Determine selection type\n",
    "        batch_bool = [s in row['batch'] for s in ['NS','G5']]\n",
    "        if any(batch_bool):\n",
    "            selec = 'NS'\n",
    "        else:\n",
    "            selec = 'PA'\n",
    "        dict_relabund['selection'].append(selec)\n",
    "        # Merge metadata and sample\n",
    "        dict_row = row | meta_row\n",
    "        # Add to main dict\n",
    "        for k, v in dict_row.items():\n",
    "            dict_relabund[k].append(v)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relabund = pd.DataFrame(dict_relabund).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out selection and size fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relabund['size'].unique(), df_relabund['selection'].unique(), df_relabund['batch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sel_size_df = defaultdict(dict)\n",
    "for sel in df_relabund['selection'].unique():\n",
    "    bool_sel = df_relabund['selection'] == sel\n",
    "    for size in df_relabund['size'].unique():\n",
    "        sz = size\n",
    "        if not size:\n",
    "            sz = 'none'\n",
    "        bool_size = df_relabund['size'] == size\n",
    "        df = df_relabund[bool_sel & bool_size]\n",
    "        dict_sel_size_df[sel][sz] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert assm_sample to cruise-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['cruise'] = [assm[:2] for assm in metadata.assembly]\n",
    "metadata.cruise.unique(), metadata.assembly.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sam_csam = {}\n",
    "for i, row in metadata.iterrows():\n",
    "    if (row.cruise not in ['D1', 'G5']) and (row.assembly not in ['G3PA.diel', 'G3PA.PM']):\n",
    "        csam = f\"{row.cruise}-{row.latitude}\"\n",
    "    else:\n",
    "        csam = row.assm_sample\n",
    "    dict_sam_csam[row.assm_sample] = csam\n",
    "dict_sam_csam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruise-location profiles for each taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sel_size_meanstd = defaultdict(dict)\n",
    "for sel, dict_size_df in dict_sel_size_df.items():\n",
    "    for size, df in dict_size_df.items():\n",
    "        df = df.copy()\n",
    "        print(sel, size, df.shape)\n",
    "        if df.shape[0] > 0:\n",
    "            df['frac_total_estcounts'] = df['frac_total_estcounts'].astype(float)\n",
    "            mean_assm_sample = df.groupby(\n",
    "                ['taxid','assm_sample']\n",
    "            )['frac_total_estcounts'].mean().unstack(level=0).fillna(0)\n",
    "            std_assm_sample = df.groupby(\n",
    "                ['taxid','assm_sample']\n",
    "            )['frac_total_estcounts'].std().unstack(level=0).fillna(0)\n",
    "            mean_assm_sample.index = [dict_sam_csam[s] for s in mean_assm_sample.index]\n",
    "            std_assm_sample.index = [dict_sam_csam[s] for s in std_assm_sample.index]\n",
    "            dict_sel_size_meanstd[sel][size] = [mean_assm_sample, std_assm_sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate taxon location profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot linkages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plots\n",
    "dict_sel_size_t = {\n",
    "    'PA': {\n",
    "        'none': 20,\n",
    "        '3.0um': 25,\n",
    "        '0.2um': 28,\n",
    "    },\n",
    "    'NS': {\n",
    "        'none': 20,\n",
    "        '3.0um': 18,\n",
    "        '0.2um': 35,\n",
    "    }\n",
    "}\n",
    "crit = 'maxclust'\n",
    "max_sam = 10\n",
    "\n",
    "filt_frac = 0.01 # taxa must have more than this fraction when summed across samples\n",
    "\n",
    "corr_method = 'pearson'\n",
    "\n",
    "# spatial plots\n",
    "ft0 = 8\n",
    "ft1 = 12\n",
    "plt.rcParams['font.size'] = ft1\n",
    "dims_sub_lat = (22,12)\n",
    "dims_exp = (16,12)\n",
    "dict_cruise_j = {\n",
    "    'G1': 0,\n",
    "    'G2': 1,\n",
    "    'G3': 2,\n",
    "    'D1PA': 0,\n",
    "    'G3PA.diel': 1,\n",
    "    'G3PA.PM': 2,\n",
    "    'G5': 3,\n",
    "}\n",
    "\n",
    "for sel, dict_size_dfs in dict_sel_size_meanstd.items():\n",
    "    for size, dfs in dict_size_dfs.items():\n",
    "        mean_df = dfs[0]\n",
    "        ms = mean_df.sum(axis=0)\n",
    "        mean_df_filt = mean_df.loc[:,ms > filt_frac]\n",
    "        print(mean_df.shape, mean_df_filt.shape)\n",
    "        \n",
    "        # calculate correlation matrix\n",
    "        corr_df = mean_df_filt.corr(method=corr_method)\n",
    "        corr_df = corr_df.replace({np.nan: 0})\n",
    "        # Precalculate linkage to extract clusters later\n",
    "        link = hierarchy.linkage(distance.pdist(np.asarray(corr_df)))\n",
    "\n",
    "        t = dict_sel_size_t[sel][size]\n",
    "\n",
    "        clust = hierarchy.fcluster(link, t=t, criterion=crit)\n",
    "        nclust = np.unique(clust).shape[0]\n",
    "\n",
    "        cmap = list(plt.get_cmap('tab20').colors)\n",
    "        cmap *= math.ceil(nclust / len(cmap))\n",
    "        clorder =[]\n",
    "        for idx in hierarchy.leaves_list(link):\n",
    "            cl = clust[idx]\n",
    "            if cl not in clorder:\n",
    "                clorder.append(cl)\n",
    "        lut = dict(zip(clorder, cmap))\n",
    "        # idx_df = hierarchy.leaves_list(links[mode])\n",
    "        row_colors = [lut[cl] for cl in clust]\n",
    "\n",
    "        # make clustered heatmap\n",
    "        # using precalculated linkage\n",
    "        g = sns.clustermap(\n",
    "            corr_df.fillna(0), \n",
    "            row_linkage=link, col_linkage=link,\n",
    "            row_colors=row_colors,\n",
    "            col_colors=row_colors,\n",
    "            mask=corr_df.isna(), \n",
    "            cmap='PuOr_r', vmin=-1, vmax=1, \n",
    "            cbar_kws={'shrink':0.5, 'label':f'{corr_method}\\nCorrelation'}, \n",
    "        )\n",
    "            # xticklabels=True, yticklabels=True\n",
    "        # g = sns.clustermap(\n",
    "        #     corr_df.fillna(0), mask=corr_df.isna(), cmap='PuOr_r', vmin=-1, vmax=1, cbar_kws={'shrink':0.5, 'label':'Pearson\\nCorrelation'}, \n",
    "        #     xticklabels=True, yticklabels=True\n",
    "        # )\n",
    "        g.fig.suptitle(f\"Similarity of relative abundance between Taxa in {sel} selection and {size} filter\", y=1.02); \n",
    "        plt.show(g)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Plot clusters over space\n",
    "        clorder_trim = [cl for cl in clorder if sum(clust == cl) > 3]\n",
    "        nrows = len(clorder_trim)\n",
    "        fig_lat, axes_lat = plt.subplots(\n",
    "            nrows=nrows, \n",
    "            ncols=3, \n",
    "            sharex=True, \n",
    "            figsize=(20,nrows*1.5)\n",
    "        )\n",
    "        fig_exp, axes_exp = plt.subplots(\n",
    "            nrows=nrows,\n",
    "            ncols=4, \n",
    "            figsize=(20,nrows)\n",
    "        )\n",
    "        profile_df = mean_df_filt.copy()\n",
    "        profile_df = profile_df / profile_df.max(axis=0)\n",
    "        profile_df = profile_df.T\n",
    "        i = 0\n",
    "        for ic, cl in enumerate(clorder):\n",
    "            bool_cl = clust == cl\n",
    "            if sum(bool_cl) > 3:\n",
    "                dict_cruise_loc_weigts = defaultdict(dict)\n",
    "\n",
    "                profile_cl = profile_df[bool_cl]\n",
    "                for csam in profile_cl.columns:\n",
    "                    cruise, info = csam.split('-',1)\n",
    "                    if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "                        info = float(info.strip('deg'))\n",
    "                    vals = profile_cl[csam].values\n",
    "                    if len(vals.shape) > 1:\n",
    "                        vals = np.mean(vals, axis=1)\n",
    "                    dict_cruise_loc_weigts[cruise][info] = vals\n",
    "\n",
    "                # Plot each cruise separately\n",
    "                for cruise, dict_loc_weights in dict_cruise_loc_weigts.items():\n",
    "                    dfc = pd.DataFrame(dict_loc_weights)\n",
    "                    j = dict_cruise_j[cruise]\n",
    "                    if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "                        ax = axes_lat[i,j]\n",
    "                        # fig, ax = plt.subplots(figsize=dims_lat)\n",
    "                        bp = ax.boxplot(dfc.values, positions=dfc.columns, patch_artist=True)\n",
    "                        # dfc.boxplot(ax=ax, positions=dfc.columns)\n",
    "                        # ax.set_xlim(23,43)\n",
    "                        # ax.set_xlabel('Latitude', fontsize=ft1)\n",
    "                        if i == nrows - 1:\n",
    "                            xticks = np.arange(20,45,5).astype(int)\n",
    "                            ax.set_xticks(xticks)\n",
    "                            ax.set_xticklabels(xticks)\n",
    "                            ax.tick_params(axis='both', labelsize=ft1) \n",
    "\n",
    "                    else:\n",
    "                        ax = axes_exp[i,j]\n",
    "                        # fig, ax = plt.subplots(figsize=dims_exp)\n",
    "                        dfc = dfc.sort_index()\n",
    "                        bp = ax.boxplot(dfc.values, patch_artist=True)\n",
    "                        if i == nrows - 1:\n",
    "                            xticks = np.arange(len(dfc.columns)) + 1\n",
    "                            ax.set_xticks(xticks)\n",
    "                            ax.set_xticklabels(dfc.columns, rotation=90)\n",
    "                            ax.tick_params(axis='both', labelsize=ft1)\n",
    "                        else:\n",
    "                            ax.set_xticks([])\n",
    "\n",
    "                    # Set colors\n",
    "                    color = cmap[ic]\n",
    "                    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']:\n",
    "                            plt.setp(bp[item], color=color)\n",
    "                    # plt.setp(box1[\"boxes\"], facecolor=c2)\n",
    "                    plt.setp(bp[\"fliers\"], markeredgecolor=color)\n",
    "\n",
    "                    if j == 2:\n",
    "                        ax.set_ylabel(f'Cluster {cl}', rotation=0, fontsize=ft0, ha='left')\n",
    "                        ax.yaxis.set_label_position(\"right\")\n",
    "                        # dfc.boxplot(ax=ax)\n",
    "                        # ax.set_xlabel('Experiment', fontsize=ft1)\n",
    "                    # ax.set_ylim(-0.05,1.05)\n",
    "                    # ax.set_xticklabels([]); \n",
    "                    # ax.tick_params(axis='both', labelsize=ft0)\n",
    "                    ax.grid(False)\n",
    "                i += 1\n",
    "\n",
    "                    # ax.set_ylabel('Component weight', fontsize=ft1)\n",
    "                    # ax.set_title(f'{cruise} - Cluster {cl}', fontsize=ft1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlate Taxon-sel-size across cruise=sample\n",
    "\n",
    "Since G5, G3PA.PM, D1, and G3PA.diel don't have samples for each sel-size combo, I'm going to remove them here. Then each sel-size combo should have the same, or nearly the same, set of cruise-locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relabund['batch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataframe to only those batches we want\n",
    "bools = np.ones(df_relabund.shape[0])\n",
    "batchs = df_relabund['batch'].values\n",
    "for b in ['D1', 'G5.RR', 'G5.mix','G3PA.diel','G3PA.PM']:\n",
    "# for b in ['D1', 'G5.RR', 'G5.mix','G3PA.diel','G3PA.PM','G2PA','G2NS','G1PA','G1NS','G3NS']:\n",
    "    bools *= (batchs != b)\n",
    "# bools *= (df_relabund['size'] == '3.0um')\n",
    "df_relabund_transct = df_relabund.copy()\n",
    "df_relabund_transct = df_relabund_transct[bools.astype(bool)]\n",
    "# Give a new taxon-selection-size name\n",
    "df_relabund_transct['taxid_sel_size'] = (\n",
    "    df_relabund_transct['selection'].astype(str) \n",
    "    + '-' + df_relabund_transct['size'].astype(str) \n",
    "    + '-' + df_relabund_transct['taxid'].astype(str) \n",
    ")\n",
    "\n",
    "df_relabund.shape, df_relabund_transct.shape, df_relabund_transct['taxid_sel_size'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out selection and size fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relabund_transct['size'].unique(), df_relabund_transct['selection'].unique(), df_relabund_transct['batch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sel_size_df = defaultdict(dict)\n",
    "for sel in df_relabund_transct['selection'].unique():\n",
    "    bool_sel = df_relabund_transct['selection'] == sel\n",
    "    for size in df_relabund_transct['size'].unique():\n",
    "        sz = size\n",
    "        if not size:\n",
    "            sz = 'none'\n",
    "        bool_size = df_relabund_transct['size'] == size\n",
    "        df = df_relabund_transct[bool_sel & bool_size]\n",
    "        dict_sel_size_df[sel][sz] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean across replicates and pivot the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_mean = []\n",
    "list_df_std = []\n",
    "for sel, dict_size_df in dict_sel_size_df.items():\n",
    "    for size, df in dict_size_df.items():\n",
    "        df = df.copy()\n",
    "        print(sel, size, df.shape)\n",
    "        if df.shape[0] > 0:\n",
    "            df['frac_total_estcounts'] = df['frac_total_estcounts'].astype(float)\n",
    "            mean_assm_sample = df.groupby(\n",
    "                ['taxid_sel_size','assm_sample']\n",
    "            )['frac_total_estcounts'].mean().unstack(level=0).fillna(0)\n",
    "            std_assm_sample = df.groupby(\n",
    "                ['taxid_sel_size','assm_sample']\n",
    "            )['frac_total_estcounts'].std().unstack(level=0).fillna(0)\n",
    "            mean_assm_sample.index = [dict_sam_csam[s] for s in mean_assm_sample.index]\n",
    "            std_assm_sample.index = [dict_sam_csam[s] for s in std_assm_sample.index]\n",
    "            list_df_mean.append(mean_assm_sample)\n",
    "            list_df_std.append(std_assm_sample)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge on cruise-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_sel_sz_csam_profile = pd.concat(list_df_mean, axis=1)\n",
    "tax_sel_sz_csam_profile.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot linkages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = 2.75\n",
    "criterion = 'distance'\n",
    "\n",
    "corr_method = 'pearson'\n",
    "\n",
    "filt_frac = 0.01 # taxa must have more than this fraction when summed across samples\n",
    "ms = tax_sel_sz_csam_profile.sum(axis=0)\n",
    "mean_df_filt = tax_sel_sz_csam_profile.loc[:,ms > filt_frac]\n",
    "\n",
    "# calculate correlation matrix\n",
    "corr_df = mean_df_filt.corr(method=corr_method)\n",
    "corr_df = corr_df.replace({np.nan: 0})\n",
    "# Precalculate linkage to extract clusters later\n",
    "link = hierarchy.linkage(distance.pdist(np.asarray(corr_df)))\n",
    "\n",
    "clust = hierarchy.fcluster(link, t=t, criterion=criterion)\n",
    "nclust = np.unique(clust).shape[0]\n",
    "\n",
    "cmap = list(plt.get_cmap('tab20').colors)\n",
    "cmap *= math.ceil(nclust / len(cmap))\n",
    "clorder =[]\n",
    "for idx in hierarchy.leaves_list(link):\n",
    "    cl = clust[idx]\n",
    "    if cl not in clorder:\n",
    "        clorder.append(cl)\n",
    "lut = dict(zip(clorder, cmap))\n",
    "# idx_df = hierarchy.leaves_list(links[mode])\n",
    "row_colors = [lut[cl] for cl in clust]\n",
    "\n",
    "# make clustered heatmap\n",
    "# using precalculated linkage\n",
    "g = sns.clustermap(\n",
    "    corr_df.fillna(0), \n",
    "    row_linkage=link, col_linkage=link,\n",
    "    row_colors=row_colors,\n",
    "    col_colors=row_colors,\n",
    "    mask=corr_df.isna(), \n",
    "    cmap='PuOr_r', vmin=-1, vmax=1, \n",
    "    cbar_kws={'shrink':0.5, 'label':f'{corr_method}\\nCorrelation'}, \n",
    ")\n",
    "    # xticklabels=True, yticklabels=True\n",
    "# g = sns.clustermap(\n",
    "#     corr_df.fillna(0), mask=corr_df.isna(), cmap='PuOr_r', vmin=-1, vmax=1, cbar_kws={'shrink':0.5, 'label':'Pearson\\nCorrelation'}, \n",
    "#     xticklabels=True, yticklabels=True\n",
    "# )\n",
    "g.fig.suptitle(f\"Similarity of Cruise-location relative abundance across taxa-selection-size\", y=1.02); \n",
    "plt.show(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot sample profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial plots\n",
    "n_tax_filt = 5  # Only plot clusters with more than this number of taxa\n",
    "\n",
    "ft0 = 8\n",
    "ft1 = 12\n",
    "plt.rcParams['font.size'] = ft1\n",
    "dims_sub_lat = (22,12)\n",
    "dims_exp = (16,12)\n",
    "dict_cruise_j = {\n",
    "    'G1': 0,\n",
    "    'G2': 1,\n",
    "    'G3': 2,\n",
    "    'D1PA': 0,\n",
    "    'G3PA.diel': 1,\n",
    "    'G3PA.PM': 2,\n",
    "    'G5': 3,\n",
    "}\n",
    "\n",
    "# Plot clusters over space\n",
    "clorder_trim = [cl for cl in clorder if sum(clust == cl) > n_tax_filt]\n",
    "nrows = len(clorder_trim)\n",
    "fig_lat, axes_lat = plt.subplots(\n",
    "    nrows=nrows, \n",
    "    ncols=3, \n",
    "    sharex=True, \n",
    "    sharey=True,\n",
    "    figsize=(20,nrows*1.5)\n",
    ")\n",
    "# fig_exp, axes_exp = plt.subplots(\n",
    "#     nrows=nrows,\n",
    "#     ncols=4, \n",
    "#     figsize=(20,nrows)\n",
    "# )\n",
    "profile_df = mean_df_filt.copy()\n",
    "profile_df = profile_df / profile_df.max(axis=0)\n",
    "profile_df = profile_df.T\n",
    "i = 0\n",
    "for ic, cl in enumerate(clorder):\n",
    "    bool_cl = clust == cl\n",
    "    if sum(bool_cl) > n_tax_filt:\n",
    "        dict_cruise_loc_weigts = defaultdict(dict)\n",
    "\n",
    "        profile_cl = profile_df[bool_cl]\n",
    "        for csam in profile_cl.columns:\n",
    "            cruise, info = csam.split('-',1)\n",
    "            if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "                info = float(info.strip('deg'))\n",
    "            vals = profile_cl[csam].values\n",
    "            if len(vals.shape) > 1:\n",
    "                vals = np.mean(vals, axis=1)\n",
    "            dict_cruise_loc_weigts[cruise][info] = vals\n",
    "\n",
    "        # Plot each cruise separately\n",
    "        for cruise, dict_loc_weights in dict_cruise_loc_weigts.items():\n",
    "            dfc = pd.DataFrame(dict_loc_weights)\n",
    "            j = dict_cruise_j[cruise]\n",
    "            if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "                ax = axes_lat[i,j]\n",
    "                # fig, ax = plt.subplots(figsize=dims_lat)\n",
    "                bp = ax.boxplot(dfc.values, positions=dfc.columns, patch_artist=True)\n",
    "                # dfc.boxplot(ax=ax, positions=dfc.columns)\n",
    "                # ax.set_xlim(23,43)\n",
    "                # ax.set_xlabel('Latitude', fontsize=ft1)\n",
    "                if i == nrows - 1:\n",
    "                    xticks = np.arange(20,45,5).astype(int)\n",
    "                    ax.set_xticks(xticks)\n",
    "                    ax.set_xticklabels(xticks)\n",
    "                    ax.tick_params(axis='both', labelsize=ft1) \n",
    "\n",
    "            # else:\n",
    "            #     ax = axes_exp[i,j]\n",
    "            #     # fig, ax = plt.subplots(figsize=dims_exp)\n",
    "            #     dfc = dfc.sort_index()\n",
    "            #     bp = ax.boxplot(dfc.values, patch_artist=True)\n",
    "            #     if i == nrows - 1:\n",
    "            #         xticks = np.arange(len(dfc.columns)) + 1\n",
    "            #         ax.set_xticks(xticks)\n",
    "            #         ax.set_xticklabels(dfc.columns, rotation=90)\n",
    "            #         ax.tick_params(axis='both', labelsize=ft1)\n",
    "            #     else:\n",
    "            #         ax.set_xticks([])\n",
    "\n",
    "            # Set colors\n",
    "            color = cmap[ic]\n",
    "            for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']:\n",
    "                    plt.setp(bp[item], color=color)\n",
    "            # plt.setp(box1[\"boxes\"], facecolor=c2)\n",
    "            plt.setp(bp[\"fliers\"], markeredgecolor=color)\n",
    "\n",
    "            if j == 2:\n",
    "                ax.set_ylabel(f'Cluster {cl}', rotation=0, fontsize=ft0, ha='left')\n",
    "                ax.yaxis.set_label_position(\"right\")\n",
    "                # dfc.boxplot(ax=ax)\n",
    "                # ax.set_xlabel('Experiment', fontsize=ft1)\n",
    "            # ax.set_ylim(-0.05,1.05)\n",
    "            # ax.set_xticklabels([]); \n",
    "            # ax.tick_params(axis='both', labelsize=ft0)\n",
    "            ax.grid(False)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print taxa for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi.get_taxid_translator([35677])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = '/scratch/bgrodner/relative_abundance/metat_search_results/plots/relabund_tax_clusters.txt'\n",
    "with open(out_fn, 'w') as f:\n",
    "    for ic, cl in enumerate(clorder):\n",
    "        bool_cl = clust == cl\n",
    "        if sum(bool_cl) > n_tax_filt:\n",
    "            print('Cluster: ',cl)\n",
    "            f.write(f'\\nCluster: {cl}\\n')\n",
    "            profile_cl = profile_df[bool_cl]\n",
    "            # print('\\t', profile_cl.index)\n",
    "            dict_tax_selsz = defaultdict(list)\n",
    "            for sel_sz_tax in profile_cl.index:\n",
    "                sel, sz, tax = sel_sz_tax.split('-')\n",
    "                dict_tax_selsz[tax].append(f'{sel}-{sz}')\n",
    "            tree_ = ncbi.get_topology(list(dict_tax_selsz.keys()))\n",
    "            for n in tree_.traverse():\n",
    "                tax = n.name\n",
    "                selszs = set(dict_tax_selsz[tax])\n",
    "                prp = ''\n",
    "                for s in selszs:\n",
    "                    prp += f'{s},'\n",
    "                prp = prp[:-1]\n",
    "                n.add_props(selsizes=prp)\n",
    "            treestr = tree_.to_str(props=['sci_name','selsizes'])\n",
    "            print(treestr)\n",
    "            f.write(treestr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = mean_assm_sample.mean(axis=0)\n",
    "ms = mean_assm_sample.std(axis=0)\n",
    "\n",
    "mm.sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Cyanophyceae'\n",
    "t = ncbi.get_name_translator([name])[name][0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial plots\n",
    "ft0 = 8\n",
    "ft1 = 12\n",
    "plt.rcParams['font.size'] = ft1\n",
    "dims_sub_lat = (22,12)\n",
    "dims_exp = (16,12)\n",
    "dict_cruise_j = {\n",
    "    'G1': 0,\n",
    "    'G2': 1,\n",
    "    'G3': 2,\n",
    "    'D1PA': 0,\n",
    "    'G3PA.diel': 1,\n",
    "    'G3PA.PM': 2,\n",
    "    'G5': 3,\n",
    "}\n",
    "\n",
    "# Plot clusters over space\n",
    "# clorder_trim = [cl for cl in clorder if sum(clust == cl) > 3]\n",
    "taxa = [f'NS-3.0um-{t}',f'NS-0.2um-{t}',f'PA-3.0um-{t}',f'PA-0.2um-{t}']\n",
    "nrows = len(taxa)\n",
    "fig_lat, axes_lat = plt.subplots(\n",
    "    nrows=nrows, \n",
    "    ncols=3, \n",
    "    sharex=True, \n",
    "    figsize=(20,nrows*1.5)\n",
    ")\n",
    "# fig_exp, axes_exp = plt.subplots(\n",
    "#     nrows=nrows,\n",
    "#     ncols=4, \n",
    "#     figsize=(20,nrows)\n",
    "# )\n",
    "# profile_df = tax_sel_sz_csam_profile[taxa].copy()\n",
    "# profile_df = tax_sel_sz_csam_profile[['NS-3.0um-1218','NS-0.2um-1218','PA-3.0um-0','PA-0.2um-0']].copy()\n",
    "# profile_df = profile_df / profile_df.max(axis=0)\n",
    "# profile_df = profile_df.T\n",
    "i = 0\n",
    "for ic, cln in enumerate(taxa):\n",
    "    # bool_cl = clust == cl\n",
    "    # if sum(bool_cl) > 3:\n",
    "    # dict_cruise_loc_weigts = defaultdict(dict)\n",
    "\n",
    "    for csam, val in tax_sel_sz_csam_profile[cln].items():\n",
    "        cruise, info = csam.split('-',1)\n",
    "        j = dict_cruise_j[cruise]\n",
    "\n",
    "        if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "            info = float(info.strip('deg'))\n",
    "        if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "            ax = axes_lat[i,j]\n",
    "            # fig, ax = plt.subplots(figsize=dims_lat)\n",
    "            bp = ax.scatter([info], [float(val)],c='k')\n",
    "            # dfc.boxplot(ax=ax, positions=dfc.columns)\n",
    "            # ax.set_xlim(23,43)\n",
    "            # ax.set_xlabel('Latitude', fontsize=ft1)\n",
    "            if i == nrows - 1:\n",
    "                xticks = np.arange(20,45,5).astype(int)\n",
    "                ax.set_xticks(xticks)\n",
    "                ax.set_xticklabels(xticks)\n",
    "                ax.tick_params(axis='both', labelsize=ft1) \n",
    "        if j == 2:\n",
    "            ax.set_ylabel(f'{cln}', rotation=0, fontsize=ft0, ha='left')\n",
    "            ax.yaxis.set_label_position(\"right\")\n",
    "    # ylim = ax.get_ylim()\n",
    "    # ax.set_ylim(0,ylim[1])\n",
    "            # dfc.boxplot(ax=ax)\n",
    "            # ax.set_xlabel('Experiment', fontsize=ft1)        \n",
    "    # for csam in profile_cl.columns:\n",
    "    #     cruise, info = csam.split('-',1)\n",
    "    #     if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "    #         info = float(info.strip('deg'))\n",
    "    #     vals = profile_cl[csam].values\n",
    "    #     if len(vals.shape) > 1:\n",
    "    #         vals = np.mean(vals, axis=1)\n",
    "    #     dict_cruise_loc_weigts[cruise][info] = vals\n",
    "\n",
    "    # # Plot each cruise separately\n",
    "    # for cruise, dict_loc_weights in dict_cruise_loc_weigts.items():\n",
    "    #     # dfc = pd.DataFrame(dict_loc_weights)\n",
    "    #     j = dict_cruise_j[cruise]\n",
    "    #     if cruise not in ['D1PA', 'G5','G3PA.diel','G3PA.PM']:\n",
    "    #         ax = axes_lat[i,j]\n",
    "    #         # fig, ax = plt.subplots(figsize=dims_lat)\n",
    "    #         bp = ax.boxplot(dfc.values, positions=dfc.columns, patch_artist=True)\n",
    "    #         # dfc.boxplot(ax=ax, positions=dfc.columns)\n",
    "    #         # ax.set_xlim(23,43)\n",
    "    #         # ax.set_xlabel('Latitude', fontsize=ft1)\n",
    "    #         if i == nrows - 1:\n",
    "    #             xticks = np.arange(20,45,5).astype(int)\n",
    "    #             ax.set_xticks(xticks)\n",
    "    #             ax.set_xticklabels(xticks)\n",
    "    #             ax.tick_params(axis='both', labelsize=ft1) \n",
    "\n",
    "    #     else:\n",
    "    #         ax = axes_exp[i,j]\n",
    "    #         # fig, ax = plt.subplots(figsize=dims_exp)\n",
    "    #         dfc = dfc.sort_index()\n",
    "    #         bp = ax.boxplot(dfc.values, patch_artist=True)\n",
    "    #         if i == nrows - 1:\n",
    "    #             xticks = np.arange(len(dfc.columns)) + 1\n",
    "    #             ax.set_xticks(xticks)\n",
    "    #             ax.set_xticklabels(dfc.columns, rotation=90)\n",
    "    #             ax.tick_params(axis='both', labelsize=ft1)\n",
    "    #         else:\n",
    "    #             ax.set_xticks([])\n",
    "\n",
    "    #     # Set colors\n",
    "    #     color = cmap[ic]\n",
    "    #     for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']:\n",
    "    #             plt.setp(bp[item], color=color)\n",
    "    #     # plt.setp(box1[\"boxes\"], facecolor=c2)\n",
    "    #     plt.setp(bp[\"fliers\"], markeredgecolor=color)\n",
    "\n",
    "    #     if j == 2:\n",
    "    #         ax.set_ylabel(f'Cluster {cl}', rotation=0, fontsize=ft0, ha='left')\n",
    "    #         ax.yaxis.set_label_position(\"right\")\n",
    "    #         # dfc.boxplot(ax=ax)\n",
    "    #         # ax.set_xlabel('Experiment', fontsize=ft1)\n",
    "    #     # ax.set_ylim(-0.05,1.05)\n",
    "    #     # ax.set_xticklabels([]); \n",
    "    #     # ax.tick_params(axis='both', labelsize=ft0)\n",
    "    #     ax.grid(False)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
